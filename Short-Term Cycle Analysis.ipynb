{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0dc7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0m/_wycfzrx38qg3p9q7p9rkbnh0000gn/T/ipykernel_8946/6088859.py:15: DtypeWarning: Columns (2,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "/var/folders/0m/_wycfzrx38qg3p9q7p9rkbnh0000gn/T/ipykernel_8946/6088859.py:15: DtypeWarning: Columns (2,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "/var/folders/0m/_wycfzrx38qg3p9q7p9rkbnh0000gn/T/ipykernel_8946/6088859.py:15: DtypeWarning: Columns (2,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "/var/folders/0m/_wycfzrx38qg3p9q7p9rkbnh0000gn/T/ipykernel_8946/6088859.py:15: DtypeWarning: Columns (3,5,6,8,10,11,12,13,14,15,17,18) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Directory where the CSV files are stored\n",
    "csv_directory = Path('/Users/owaisraza/Desktop/PWC/TFL')\n",
    "\n",
    "# Load all CSV files in the directory and sort them\n",
    "csv_files = sorted(csv_directory.glob('*.csv'))\n",
    "\n",
    "# Initialize an empty DataFrames\n",
    "df_list = []\n",
    "\n",
    "# Loop through the sorted CSV files and read each into a DataFrame\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file)\n",
    "    df['source_file'] = file.stem\n",
    "    df_list.append(df)\n",
    "\n",
    "# Concatenate all the DataFrames in the list into a single DataFrame\n",
    "combined_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Sort the combined DataFrame by the new 'source_file' column\n",
    "combined_df = combined_df.sort_values(by='source_file')\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362a56c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'Rent_ID' with a unique identifier for each row\n",
    "combined_df['Rent_ID'] = range(1, len(combined_df) + 1)\n",
    "\n",
    "combined_df_with_rent_id = combined_df.head()\n",
    "\n",
    "combined_df_with_rent_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c9a269",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Checking for missing values in the dataset\n",
    "missing_values = combined_df.isnull().sum()\n",
    "\n",
    "# Getting a description of the dataset\n",
    "data_description = combined_df.describe()\n",
    "\n",
    "# Checking the datatypes of the columns\n",
    "data_types = combined_df.dtypes\n",
    "\n",
    "missing_values, data_description, data_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038d8eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['End Date'] = pd.to_datetime(combined_df['End Date'], format='%d/%m/%Y %H:%M')\n",
    "combined_df['Start Date'] = pd.to_datetime(combined_df['Start Date'], format='%d/%m/%Y %H:%M')\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2150a6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['End date'] = pd.to_datetime(combined_df['End date'])\n",
    "combined_df['Start date'] = pd.to_datetime(combined_df['Start date'])\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f62aa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_types = combined_df.dtypes\n",
    "data_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14a580c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the 'rental_id' and 'number' columns from the dataframe\n",
    "combined_df.drop(['Rental Id', 'Number'], axis=1, inplace=True)\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b316f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine 'Start Date' and 'Start date' into a new column 'Start_Date_Combined'\n",
    "combined_df['Start_Date'] = combined_df['Start Date'].combine_first(combined_df['Start date'])\n",
    "combined_df['End_Date'] = combined_df['End Date'].combine_first(combined_df['End date'])\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f2e83a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove the original 'Start Date', 'Start date', 'End Date', and 'End date' columns\n",
    "combined_df.drop(['Start Date', 'Start date', 'End Date', 'End date'], axis=1, inplace=True)\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dae2cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combined_df.drop(['Total duration'], axis=1, inplace=True)\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c104faa8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Divide 'Total duration (ms)' by 1000 to convert to seconds and round to 0 decimal places\n",
    "combined_df['Total_duration_s'] = (combined_df['Total duration (ms)'] / 1000).round(0)\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25723ae4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Combine 'Total_duration_s' and 'Duration' into a new column 'Total_duration'\n",
    "combined_df['Total_duration'] = combined_df['Total_duration_s'].combine_first(combined_df['Duration'])\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b15c2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove original 'Duration', 'Total duration (ms)', 'Total_duration_s' columns\n",
    "combined_df.drop(['Duration', 'Total duration (ms)', 'Total_duration_s'], axis=1, inplace=True)\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17380a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Combine 'StartStation Name' and 'Start station' into a new column 'Start_Station_Name'\n",
    "combined_df['Start_Station_Name'] = combined_df['StartStation Name'].combine_first(combined_df['Start station'])\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25b564a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove the original 'StartStation Name', 'Start station'\n",
    "combined_df.drop(['StartStation Name', 'Start station'], axis=1, inplace=True)\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0248d980",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Combine 'EndStation Name' and 'End station' into a new column 'End_Station_Name'\n",
    "combined_df['End_Station_Name'] = combined_df['EndStation Name'].combine_first(combined_df['End station'])\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43469fb0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove the original 'EndStation Name', 'End station'\n",
    "combined_df.drop(['EndStation Name', 'End station'], axis=1, inplace=True)\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25de0297",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Adjust the unique_sorted function to handle NaN values and avoid type comparison errors\n",
    "def unique_sorted(series):\n",
    "    # Convert all values to strings, sort them, and then return\n",
    "    return sorted(series.astype(str).unique())\n",
    "\n",
    "# Group the dataframe by 'EndStation Name' and aggregate the unique, sorted values of 'End station number' and 'EndStation Id' columns\n",
    "end_station_mapping = combined_df.groupby('End_Station_Name').agg({\n",
    "    'End station number': unique_sorted,\n",
    "    'EndStation Id': unique_sorted\n",
    "}).reset_index()\n",
    "\n",
    "end_station_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f011f157",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove the original 'EndStation Id', 'End station number', 'Start station number','StartStation Id' \n",
    "combined_df.drop(['EndStation Id', 'End station number', 'Start station number','StartStation Id' ], axis=1, inplace=True)\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce7194e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check for the first non-null entry in the 'Bike model' column\n",
    "bike_model_start_index = combined_df['Bike model'].first_valid_index()\n",
    "\n",
    "# Display the row where bike model names start\n",
    "bike_model_row = combined_df.loc[bike_model_start_index] if bike_model_start_index is not None else \"No bike model names found\"\n",
    "bike_model_row, bike_model_start_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c234f7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the original 'Bike Id' and 'Bike number' \n",
    "combined_df.drop(['Bike Id', 'Bike number'], axis=1, inplace=True)\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd7adb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_types = combined_df.dtypes\n",
    "data_types\n",
    "# Plotting the box plot for 'Total_duration' to check outliers\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(combined_df['Total_duration'])\n",
    "plt.title('Box Plot of Total_duration')\n",
    "plt.xlabel('Total_duration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ad9b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Using IQR to filter out outliers from the 'Duration' column\n",
    "Q1 = combined_df['Total_duration'].quantile(0.25)\n",
    "Q3 = combined_df['Total_duration'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define the bounds for outliers\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Filter out outliers\n",
    "combined_df = combined_df[(combined_df['Total_duration'] >= lower_bound) & (combined_df['Total_duration'] <= upper_bound)]\n",
    "\n",
    "filtered_shape = combined_df.shape\n",
    "\n",
    "filtered_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c08ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_types = combined_df.dtypes\n",
    "data_types\n",
    "# Plot box plot for 'Total_duration'\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(combined_df['Total_duration'])\n",
    "plt.title('Box Plot of Total_duration')\n",
    "plt.xlabel('Total_duration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bc81f0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Converte duration to minutes and getting top stations\n",
    "combined_df['Duration (minutes)'] = combined_df['Total_duration'] / 60\n",
    "top_start_stations = combined_df['Start_Station_Name'].value_counts().head(20).index\n",
    "top_end_stations = combined_df['End_Station_Name'].value_counts().head(20).index\n",
    "\n",
    "# Calculate average rental duration for top start and end stations\n",
    "avg_duration_start_stations = combined_df[combined_df['Start_Station_Name'].isin(top_start_stations)] \\\n",
    "    .groupby('Start_Station_Name')['Duration (minutes)'].mean().sort_values(ascending=False)\n",
    "avg_duration_end_stations = combined_df[combined_df['End_Station_Name'].isin(top_end_stations)] \\\n",
    "    .groupby('End_Station_Name')['Duration (minutes)'].mean().sort_values(ascending=False)\n",
    "\n",
    "# Define the color palette\n",
    "color_palette = [\"#AD1B02\", \"#D85604\", \"#E88D14\", \"#F3BE26\"]\n",
    "sns.set_palette(sns.color_palette(color_palette))\n",
    "\n",
    "# Create visualizations\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Average rental duration per top start station \n",
    "plt.subplot(2, 1, 1)\n",
    "sns.barplot(y=avg_duration_start_stations.index, x=avg_duration_start_stations.values, palette=color_palette)\n",
    "plt.title('Average Rental Duration per Top Start Station')\n",
    "plt.xlabel('Average Duration (minutes)')\n",
    "plt.ylabel('Start Station Name')\n",
    "\n",
    "# Average rental duration per top end station\n",
    "plt.subplot(2, 1, 2)\n",
    "sns.barplot(y=avg_duration_end_stations.index, x=avg_duration_end_stations.values, palette=color_palette)\n",
    "plt.title('Average Rental Duration per Top End Station')\n",
    "plt.xlabel('Average Duration (minutes)')\n",
    "plt.ylabel('End Station Name')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a12238e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar graph of number of rents in hours, week and month\n",
    "combined_df['Hour of Day'] = combined_df['Start_Date'].dt.hour\n",
    "combined_df['Day of Week'] = combined_df['Start_Date'].dt.day_name()\n",
    "combined_df['Month'] = combined_df['Start_Date'].dt.month_name()\n",
    "\n",
    "# Aggregate the number of rentals by hour, day of the week, and month\n",
    "hourly_rentals = combined_df.groupby('Hour of Day').size()\n",
    "weekly_rentals = combined_df.groupby('Day of Week').size().reindex([\n",
    "    'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'\n",
    "])\n",
    "monthly_rentals = combined_df.groupby('Month').size().reindex([\n",
    "    'January', 'February', 'March', 'April', 'May', 'June',\n",
    "    'July', 'August', 'September', 'October', 'November', 'December'\n",
    "])\n",
    "\n",
    "# Plot the bar graphs\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create the bar graphs\n",
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "# Hourly rentals\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.barplot(x=hourly_rentals.index, y=hourly_rentals.values, palette=color_palette)\n",
    "plt.title('Number of Rentals by Hour of Day')\n",
    "plt.xlabel('Hour of Day')\n",
    "plt.ylabel('Number of Rentals')\n",
    "\n",
    "# Weekly rentals\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.barplot(x=weekly_rentals.index, y=weekly_rentals.values, palette=color_palette)\n",
    "plt.title('Number of Rentals by Day of Week')\n",
    "plt.xlabel('Day of Week')\n",
    "plt.ylabel('Number of Rentals')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Monthly rentals\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.barplot(x=monthly_rentals.index, y=monthly_rentals.values, palette=color_palette)\n",
    "plt.title('Number of Rentals by Month')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Number of Rentals')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356726d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the top 20 start stations by the number of rentals\n",
    "top_start_stations = combined_df['Start_Station_Name'].value_counts().head(20)\n",
    "\n",
    "# Calculate the top 20 end stations by the number of rentals\n",
    "top_end_stations = combined_df['End_Station_Name'].value_counts().head(20)\n",
    "\n",
    "# Plot the number of rentals for the top 20 start stations\n",
    "plt.figure(figsize=(14, 7))\n",
    "sns.barplot(y=top_start_stations.index, x=top_start_stations.values, palette=color_palette)\n",
    "plt.title('Top 20 Start Stations by Number of Rentals')\n",
    "plt.xlabel('Number of Rentals')\n",
    "plt.ylabel('Start Station Name')\n",
    "plt.show()\n",
    "\n",
    "# Plot the number of rentals for the top 20 end stations\n",
    "plt.figure(figsize=(14, 7))\n",
    "sns.barplot(y=top_end_stations.index, x=top_end_stations.values, palette=color_palette)\n",
    "plt.title('Top 20 End Stations by Number of Rentals')\n",
    "plt.xlabel('Number of Rentals')\n",
    "plt.ylabel('End Station Name')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ca709b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pie chart for the top 5 start stations\n",
    "top_5_start_stations = combined_df['Start_Station_Name'].value_counts().head(5)\n",
    "top_5_end_stations = combined_df['End_Station_Name'].value_counts().head(5)\n",
    "\n",
    "# Define explode to highlight Hyde Park in the pie chart\n",
    "explode = (0.1 if \"Hyde Park Corner, Hyde Park\" in top_5_start_stations else 0, \n",
    "           0.1 if \"Hyde Park Corner, Hyde Park\" in top_5_end_stations else 0,\n",
    "           0, 0, 0)\n",
    "\n",
    "# Pie chart for top 5 start stations\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.pie(top_5_start_stations, labels=top_5_start_stations.index, autopct='%1.1f%%', startangle=140, explode=explode, colors=color_palette)\n",
    "plt.title('Top 5 Stations')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f27944",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Convert 'Start Date' as datetime and extract 'Hour of Day' and 'Day of Week'\n",
    "df['Hour of Day'] = combined_df['Start_Date'].dt.hour\n",
    "df['Day of Week'] = combined_df['Start_Date'].dt.day_name()\n",
    "\n",
    "# Create a pivot table with 'Hour of Day' as columns and 'Day of Week' as rows\n",
    "pivot_table = combined_df.pivot_table(values='Rent_ID', index='Day of Week', columns='Hour of Day', aggfunc='count')\n",
    "\n",
    "# Fill NaN values with 0 and sort the days of the week\n",
    "pivot_table = pivot_table.fillna(0)\n",
    "pivot_table = pivot_table.reindex(['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])\n",
    "\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "# Define the color for the highest value using the given hex code\n",
    "color = '#AD1B02'\n",
    "\n",
    "# Create a olormap(which will represent the lowest values)\n",
    "cmap1 = LinearSegmentedColormap.from_list('custom_cmap_reversed', [color, 'white'][::-1])\n",
    "\n",
    "# Generate the heatmap with annotations\n",
    "plt.figure(figsize=(15, 8))\n",
    "sns.heatmap(pivot_table, cmap=cmap1, linewidths=.5)\n",
    "plt.title('Heatmap of Rentals by Hour and Day of Week', fontsize=16)\n",
    "plt.xlabel('Hour of Day', fontsize=12)\n",
    "plt.ylabel('Day of Week', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd0e728",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Count the occurrences of each bike model\n",
    "bike_model_counts = combined_df['Bike model'].value_counts()\n",
    "\n",
    "# Create a bar plot for the bike models\n",
    "plt.figure(figsize=(10, 6))\n",
    "bike_model_counts.plot(kind='bar')\n",
    "plt.title('Bike Model Usage')\n",
    "plt.xlabel('Bike Model')\n",
    "plt.ylabel('Number of Rentals')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f6874f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "combined_df.set_index('Start_Date', inplace=True)\n",
    "\n",
    "daily_rentals = combined_df.resample('D').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2324936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform seasonality\n",
    "decomposition_results = seasonal_decompose(daily_rentals, model='additive')\n",
    "seasonality = decomposition_results.seasonal\n",
    "\n",
    "# Plot the seasonal component\n",
    "seasonality.plot(title='Seasonal Component of the Time Series')\n",
    "plt.show()\n",
    "\n",
    "# Determine if the length of the dataset is sufficient for this.\n",
    "seasonality_length = len(seasonality.dropna())\n",
    "seasonality_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7c5c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 80% of the data for training and the last 20% for testing\n",
    "\n",
    "split_point = int(len(daily_rentals) * 0.8)\n",
    "train, test = daily_rentals[:split_point], daily_rentals[split_point:]\n",
    "\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "# Fit a SARIMA model to the training set\n",
    "sarima_model = SARIMAX(train,\n",
    "                       order=(1, 1, 1),\n",
    "                       seasonal_order=(1, 1, 1, 7),\n",
    "                       enforce_stationarity=False,\n",
    "                       enforce_invertibility=False)\n",
    "\n",
    "sarima_result = sarima_model.fit()\n",
    "\n",
    "# Summarize the SARIMA model\n",
    "sarima_summary = sarima_result.summary()\n",
    "sarima_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5877d4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "predictions = sarima_result.get_forecast(steps=len(test))\n",
    "predicted_mean = predictions.predicted_mean\n",
    "predicted_ci = predictions.conf_int()\n",
    "\n",
    "#Plot the predictions along with the observed data\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train.index, train, label='Training Data')\n",
    "plt.plot(test.index, test, label='Actual Test Data')\n",
    "plt.plot(test.index, predicted_mean, label='SARIMA Predictions')\n",
    "plt.fill_between(test.index, \n",
    "                 predicted_ci.iloc[:, 0], \n",
    "                 predicted_ci.iloc[:, 1], color='k', alpha=0.2)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Calculate error metrics such as Mean Absolute Error (MAE)\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "mae = mean_absolute_error(test, predicted_mean)\n",
    "print('The Mean Absolute Error of our forecasts is {}'.format(mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffde190a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the diagnostics\n",
    "sarima_result.plot_diagnostics(figsize=(15, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dd4b1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check for outliers by examining large residuals\n",
    "outliers = sarima_result.resid[sarima_result.resid.abs() > 3]\n",
    "\n",
    "# Display potential outliers\n",
    "outliers.plot(style='o', title='Outliers Identified in Residuals')\n",
    "plt.show()\n",
    "\n",
    "outlier_dates = outliers.index\n",
    "outlier_dates\n",
    "\n",
    "# Check for auto coorelation issue\n",
    "sarima_model_refined = SARIMAX(train,\n",
    "                               order=(1, 1, 1),\n",
    "                               seasonal_order=(2, 1, 1, 7),\n",
    "                               enforce_stationarity=False,\n",
    "                               enforce_invertibility=False)\n",
    "\n",
    "sarima_result_refined = sarima_model_refined.fit()\n",
    "\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "plot_acf(sarima_result_refined.resid, lags=20, alpha=0.05, title='ACF of Refined Model Residuals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565253d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "predictions = sarima_result_refined.get_forecast(steps=len(test))\n",
    "predicted_mean = predictions.predicted_mean\n",
    "predicted_ci = predictions.conf_int()\n",
    "\n",
    "# Plot the predictions along with the observed data\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train.index, train, label='Training Data')\n",
    "plt.plot(test.index, test, label='Actual Test Data')\n",
    "plt.plot(test.index, predicted_mean, label='SARIMA Predictions')\n",
    "plt.fill_between(test.index, \n",
    "                 predicted_ci.iloc[:, 0], \n",
    "                 predicted_ci.iloc[:, 1], color='k', alpha=0.2)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Calculate error metrics such as Mean Absolute Error (MAE)\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "mae = mean_absolute_error(test, predicted_mean)\n",
    "print('The Mean Absolute Error of our forecasts is {}'.format(mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e08c854",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "p = d = q = range(0, 3)  \n",
    "pdq = list(itertools.product(p, d, q))\n",
    "seasonal_pdq = [(x[0], x[1], x[2], 7) for x in pdq]\n",
    "\n",
    "best_aic = float('inf')\n",
    "best_pdq = None\n",
    "best_seasonal_pdq = None\n",
    "best_model = None\n",
    "\n",
    "for param in pdq:\n",
    "    for param_seasonal in seasonal_pdq:\n",
    "        try:\n",
    "            model = SARIMAX(train,\n",
    "                            order=param,\n",
    "                            seasonal_order=param_seasonal,\n",
    "                            enforce_stationarity=False,\n",
    "                            enforce_invertibility=False)\n",
    "            results = model.fit()\n",
    "            \n",
    "            if results.aic < best_aic:\n",
    "                best_aic = results.aic\n",
    "                best_pdq = param\n",
    "                best_seasonal_pdq = param_seasonal\n",
    "                best_model = results\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "print('Best SARIMA{}x{} - AIC:{}'.format(best_pdq, best_seasonal_pdq, best_aic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cee652e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the SARIMA model with the best parameters\n",
    "best_sarima_model = SARIMAX(train,\n",
    "                            order=best_pdq,\n",
    "                            seasonal_order=best_seasonal_pdq,\n",
    "                            enforce_stationarity=False,\n",
    "                            enforce_invertibility=False)\n",
    "best_sarima_result = best_sarima_model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009f2c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecast on test set\n",
    "forecast = best_sarima_result.get_forecast(steps=len(test))\n",
    "forecast_mean = forecast.predicted_mean\n",
    "forecast_ci = forecast.conf_int()\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.plot(train.index, train, label='Training Data')\n",
    "plt.plot(test.index, test, label='Actual Test Data')\n",
    "plt.plot(test.index, forecast_mean, label='SARIMA Predictions')\n",
    "plt.fill_between(test.index, forecast_ci.iloc[:, 0], forecast_ci.iloc[:, 1], color='k', alpha=0.2)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae762391",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Calculate MSE and RMSE\n",
    "mse = mean_squared_error(test, forecast_mean)\n",
    "rmse = mse ** 0.5\n",
    "print(f'The Mean Squared Error of our forecasts is {mse}')\n",
    "print(f'The Root Mean Squared Error of our forecasts is {rmse}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
